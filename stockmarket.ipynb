{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnnxefBT2y9Or/+wMk4J4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshavardhan2911/stockmarket29112002/blob/main/stockmarket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Akry-mVBeUvp",
        "outputId": "4bb335c8-d768-47bd-87f3-a6045ce232f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsMNiNRYdUmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d881b120-ca98-452b-a1a6-bb1f467ff987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import re\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stopword=set(stopwords.words('english'))\n",
        "\n",
        "# Function to initialize the Reddit API client\n",
        "def initialize_reddit_client():\n",
        "    return praw.Reddit(\n",
        "        client_id=\"wjF9jS9lR9f4tTItCZ-XQA\",\n",
        "        client_secret=\"k0Vb7WJxY8xvtWM-m2N-j1ZihFmkdA\",\n",
        "        user_agent=\"Harshavardhanv1.0ImplementNearby3261 \" ,\n",
        "        check_for_async=False\n",
        "    )\n",
        "\n",
        "# Function to scrape posts from a subreddit\n",
        "def scrape_reddit_subreddit(reddit, subreddit_name, limit=1500):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # List to hold scraped data\n",
        "    posts_data = []\n",
        "\n",
        "    for post in subreddit.new(limit=limit):\n",
        "        posts_data.append({\n",
        "            \"title\": post.title,\n",
        "            \"score\": post.score,\n",
        "            \"id\": post.id,\n",
        "            \"url\": post.url,\n",
        "            \"created_utc\": post.created_utc,\n",
        "            \"num_comments\": post.num_comments,\n",
        "            \"body\": post.selftext,\n",
        "        })\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    posts_df = pd.DataFrame(posts_data)\n",
        "    return posts_df\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "8TlAp31uwCye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return 1 if analysis.sentiment.polarity > 0 else 0"
      ],
      "metadata": {
        "id": "MO4RD2OEfdCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  def check_stock_sentiment(stock_name, data):\n",
        "      relevant_posts = data[data['title'].str.contains(stock_name, case=False, na=False)]\n",
        "\n",
        "      if relevant_posts.empty:\n",
        "          return f\"No relevant posts found for stock: {stock_name}\"\n",
        "\n",
        "      positive_posts = relevant_posts[relevant_posts['sentiment'] == 1].shape[0]\n",
        "      negative_posts = relevant_posts[relevant_posts['sentiment'] == 0].shape[0]\n",
        "\n",
        "      if positive_posts > negative_posts:\n",
        "          return f\"Sentiment for {stock_name}: Increasing (Positive sentiment: {positive_posts}, Negative sentiment: {negative_posts})\"\n",
        "      elif positive_posts < negative_posts:\n",
        "          return f\"Sentiment for {stock_name}: Decreasing (Positive sentiment: {positive_posts}, Negative sentiment: {negative_posts})\"\n",
        "      else:\n",
        "          return f\"Sentiment for {stock_name}: Neutral (Positive sentiment: {positive_posts}, Negative sentiment: {negative_posts})\"\n",
        "\n"
      ],
      "metadata": {
        "id": "LuLDX390e37S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize Reddit client\n",
        "    reddit = initialize_reddit_client()\n",
        "\n",
        "    subreddit_name = \"wallstreetbets\"\n",
        "    limit = 1500\n",
        "\n",
        "    # Scrape subreddit data\n",
        "    posts_df = scrape_reddit_subreddit(reddit, subreddit_name, limit)\n",
        "\n",
        "    # Save the data to a CSV file\n",
        "    output_file = f\"{subreddit_name}_posts.csv\"\n",
        "    posts_df.to_csv(output_file, index=False)\n",
        "    print(f\"Scraped data saved to {output_file}\")\n",
        "\n",
        "    # Load the data back for sentiment analysis and modeling\n",
        "    data = pd.read_csv(output_file)\n",
        "    #\n",
        "    data[\"title\"] = data[\"title\"].apply(clean_text)\n",
        "    # Add a sentiment column based on post titles\n",
        "    data[\"sentiment\"] = data[\"title\"].apply(analyze_sentiment)\n",
        "\n",
        "\n",
        "    # Prepare data for machine learning\n",
        "    X = data[\"title\"]  # Features (post titles)\n",
        "    y = data[\"sentiment\"]  # Labels (sentiment)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Vectorize text data\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_vect = vectorizer.fit_transform(X_train)\n",
        "    X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_vect)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data.head(10)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9duh4P9jjHIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_name = input(\"Enter your favorite stock to analyze sentiment: \")\n",
        "sentiment_result = check_stock_sentiment(stock_name, data)\n",
        "print(sentiment_result)"
      ],
      "metadata": {
        "id": "RIQE4_lij_z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644bf768-884b-40a5-8eb9-3ae5584d644c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your favorite stock to analyze sentiment: tesla\n",
            "Sentiment for tesla: Decreasing (Positive sentiment: 2, Negative sentiment: 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " accuracy = accuracy_score(y_test, y_pred)\n",
        " report = classification_report(y_test, y_pred)\n",
        "\n",
        " print(f\"Accuracy: {accuracy}\")\n",
        " print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW5xeYo7joYZ",
        "outputId": "907e0aeb-f294-4cfe-8e9f-4ac20ef3203b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.926829268292683\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.99      0.96       139\n",
            "           1       0.93      0.56      0.70        25\n",
            "\n",
            "    accuracy                           0.93       164\n",
            "   macro avg       0.93      0.78      0.83       164\n",
            "weighted avg       0.93      0.93      0.92       164\n",
            "\n"
          ]
        }
      ]
    }
  ]
}